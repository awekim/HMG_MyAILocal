{"cells":[{"cell_type":"markdown","metadata":{"id":"WEHXmLJJwl2l"},"source":["# 나만의 대화형 인공지능 활용법\n","\n","#### 한동대학교 AI융합교육원 김경외 교수\n","\n","##### 출처: https://github.com/camenduru/text-generation-webui-colab"]},{"cell_type":"markdown","metadata":{"id":"jDJMoRPww5i3"},"source":["### 활용 방법\n","- 오른쪽 상단의 런타임 유형이 T4인지 확인합니다 (무료계정일 경우).\n","- 아래 코드 박스를 작동시킵니다. (ctrl + enter)\n","- 중간에 멈추면, 당황하지 말고 다시 코드를 작동 시킵니다.\n","- Public URL을 클릭하면 Web-UI로 연결됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH8yY9h2yMgz"},"outputs":[],"source":["# web-ui 구동에 필요한 패키지를 설치합니다.\n","%cd /content\n","!apt-get -y install -qq aria2\n","\n","# GitHub 레포지토리를 다운로드합니다.\n","!git clone -b v1.8 https://github.com/camenduru/text-generation-webui\n","\n","# 구글 코랩의 경로를 설정합니다. 아래 경로를 통해서 필요한 데이터를 업로드 합니다.\n","%cd /content/text-generation-webui\n","# requirements.txt 파일에 명시된 패키지를 설치합니다.\n","!pip install -r requirements.txt\n","\n","# HuggingFace를 통해 LLaMA-2 모형을 불러옵니다.\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/model-00001-of-00002.safetensors -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o model-00001-of-00002.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/model-00002-of-00002.safetensors -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o model-00002-of-00002.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/model.safetensors.index.json -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o model.safetensors.index.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o special_tokens_map.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/tokenizer.model -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o tokenizer.model\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o tokenizer_config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/config.json -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/generation_config.json -d /content/text-generation-webui/models/Llama-2-7b-chat-hf -o generation_config.json\n","\n","%cd /content/text-generation-webui\n","!python server.py --share --chat --model /content/text-generation-webui/models/Llama-2-7b-chat-hf"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1rH00Qvj6WussHOfCxRbNxNAQmfgEV_Vz","authorship_tag":"ABX9TyPIvq6zol/o2l8xlc8Qd0Xz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}